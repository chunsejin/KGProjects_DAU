{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Ch5. Centrality Algorithms",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Quick setup for pySpark and GraphFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!tar xf /content/spark-3.0.3-bin-hadoop2.7.tgz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget -q https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.0-s_2.12/graphframes-0.8.2-spark3.0-s_2.12.jar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## install graphframe library on Colab\n",
    "!mv /content/graphframes-0.8.2-spark3.0-s_2.12.jar /content/spark-3.0.3-bin-hadoop2.7/jars/graphframes-0.8.2-spark3.0-s_2.12.jar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip -q install findspark pyspark graphframes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"Colab\")\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# confirm the spark installation\n",
    "spark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.environ[\"HADOOP_HOME\"] = os.environ[\"SPARK_HOME\"]\n",
    "\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"notebook\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages graphframes:graphframes:0.8.2-spark3.0-s_2.12 pyspark-shell\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from graphframes import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_spark(app_name=\"HelloWorldApp\", execution_mode=\"local[*]\"):\n",
    "  spark = SparkSession.builder.master(execution_mode).appName(app_name).getOrCreate()\n",
    "  sc = spark.sparkContext\n",
    "  return spark, sc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example: GraphFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "_, sc = init_spark()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "## the rest of this code (down below) comes from: https://graphframes.github.io/graphframes/docs/_site/quick-start.html#getting-started-with-apache-spark-and-spark-packages\n",
    "\n",
    "# Create a Vertex DataFrame with unique ID column \"id\"\n",
    "v = sqlContext.createDataFrame([\n",
    "  (\"a\", \"Alice\", 34),\n",
    "  (\"b\", \"Bob\", 36),\n",
    "  (\"c\", \"Charlie\", 30),\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Create an Edge DataFrame with \"src\" and \"dst\" columns\n",
    "e = sqlContext.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"c\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "# Create a GraphFrame\n",
    "from graphframes import *\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "# Query: Get in-degree of each vertex.\n",
    "g.inDegrees.show()\n",
    "\n",
    "# Query: Count the number of \"follow\" connections in the graph.\n",
    "g.edges.filter(\"relationship = 'follow'\").count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing the Data into Apache Spark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!wget -q https://github.com/chunsejin/KGProjects_DAU/raw/master/data/social-nodes.csv\n",
    "!wget -q https://github.com/chunsejin/KGProjects_DAU/raw/master/data/social-relationships.csv\n",
    "!mv social-nodes.csv data\n",
    "!mv social-relationships.csv data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "from pyspark import SparkContext"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "v = spark.read.csv(\"data/social-nodes.csv\", header=True)\n",
    "e = spark.read.csv(\"data/social-relationships.csv\", header=True)\n",
    "g = GraphFrame(v, e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Degree Centrality with Apache Spark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_degree = g.degrees\n",
    "in_degree = g.inDegrees\n",
    "out_degree = g.outDegrees\n",
    "(total_degree.join(in_degree, \"id\", how=\"left\")\n",
    " .join(out_degree, \"id\", how=\"left\")\n",
    " .fillna(0)\n",
    " .sort(\"inDegree\", ascending=False)\n",
    " .show())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Closeness Centrality with Apache Spark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from graphframes.lib import AggregateMessages as AM\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from operator import itemgette"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def collect_paths(paths):\n",
    "    return F.collect_set(paths)\n",
    "\n",
    "collect_paths_udf = F.udf(collect_paths, ArrayType(StringType()))\n",
    "paths_type = ArrayType(\n",
    "    StructType([StructField(\"id\", StringType()), StructField(\"distance\",\n",
    "\n",
    "def flatten(ids):\n",
    "    flat_list = [item for sublist in ids for item in sublist]\n",
    "    return list(dict(sorted(flat_list, key=itemgetter(0))).items())\n",
    "\n",
    "flatten_udf = F.udf(flatten, paths_type)\n",
    "\n",
    "def new_paths(paths, id):\n",
    "    paths = [{\"id\": col1, \"distance\": col2 + 1} for col1,\n",
    "                                                    col2 in paths if col1 != id]\n",
    "    paths.append({\"id\": id, \"distance\": 1})\n",
    "    return paths\n",
    "\n",
    "new_paths_udf = F.udf(new_paths, paths_type)\n",
    "\n",
    "def merge_paths(ids, new_ids, id):\n",
    "    joined_ids = ids + (new_ids if new_ids else [])\n",
    "    merged_ids = [(col1, col2) for col1, col2 in joined_ids if col1 != id]\n",
    "    best_ids = dict(sorted(merged_ids, key=itemgetter(1), reverse=True))\n",
    "    return [{\"id\": col1, \"distance\": col2} for col1, col2 in best_ids.items()]\n",
    "\n",
    "merge_paths_udf = F.udf(merge_paths, paths_type)\n",
    "\n",
    "def calculate_closeness(ids):\n",
    "    nodes = len(ids)\n",
    "    total_distance = sum([col2 for col1, col2 in ids])\n",
    "    return 0 if total_distance == 0 else nodes * 1.0 / total_distance\n",
    "\n",
    "closeness_udf = F.udf(calculate_closeness, DoubleType())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vertices = g.vertices.withColumn(\"ids\", F.array())\n",
    "cached_vertices = AM.getCachedDataFrame(vertices)\n",
    "g2 = GraphFrame(cached_vertices, g.edges)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(0, g2.vertices.count()):\n",
    "    msg_dst = new_paths_udf(AM.src[\"ids\"], AM.src[\"id\"])\n",
    "    msg_src = new_paths_udf(AM.dst[\"ids\"], AM.dst[\"id\"])\n",
    "    agg = g2.aggregateMessages(F.collect_set(AM.msg).alias(\"agg\"),\n",
    "                               sendToSrc=msg_src, sendToDst=msg_dst)\n",
    "    res = agg.withColumn(\"newIds\", flatten_udf(\"agg\")).drop(\"agg\")\n",
    "    new_vertices = (g2.vertices.join(res, on=\"id\", how=\"left_outer\")\n",
    "                    .withColumn(\"mergedIds\", merge_paths_udf(\"ids\", \"newIds\",\n",
    "                                                             \"id\")).drop(\"ids\", \"newIds\")\n",
    "                    .withColumnRenamed(\"mergedIds\", \"ids\"))\n",
    "    cached_new_vertices = AM.getCachedDataFrame(new_vertices)\n",
    "    g2 = GraphFrame(cached_new_vertices, g2.edges)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(g2.vertices\n",
    " .withColumn(\"closeness\", closeness_udf(\"ids\"))\n",
    " .sort(\"closeness\", ascending=False)\n",
    " .show(truncate=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PageRank with Apache Spark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = g.pageRank(resetProbability=0.15, maxIter=20)\n",
    "results.vertices.sort(\"pagerank\", ascending=False).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = g.pageRank(resetProbability=0.15, tol=0.01)\n",
    "results.vertices.sort(\"pagerank\", ascending=False).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Appendix: Timing and Profling\n",
    "%time: Time the execution of a single statement\n",
    "\n",
    "%timeit: Time repeated execution of a single statement for more accuracy\n",
    "\n",
    "%prun: Run code with the profiler\n",
    "\n",
    "%lprun: Run code with the line-by-line profiler\n",
    "\n",
    "%memit: Measure the memory use of a single statement\n",
    "\n",
    "%mprun: Run code with the line-by-line memory profile"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}